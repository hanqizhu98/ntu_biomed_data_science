# -*- coding: utf-8 -*-
"""q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pDqqBIm3t88BW-mwWyakp8TOEzAPeDtD
"""

from tensorflow import keras
from keras.models import Sequential
import tensorflow as tf
import numpy as np



train_set = []
first = True
with open("/content/train_data.txt", "r") as f:
  for line in f:
    # remove newline terminal
    line = line.strip("\n")
    # split line with tab symbol
    line = line.split("\t")
    # convert string to number
    for i in range(len(line)):
      line[i] = float(line[i])
    
    line = np.expand_dims(np.array(line), axis=0)
    # print(np.array(line).shape)
    if (first == True):
      train_set = line
      first = False
    else:
      train_set = np.concatenate((train_set, line), axis = 0)

print(train_set.shape)

train_label = []
first = True
with open("/content/train_truth.txt", "r") as f:
  for line in f:
    # remove newline terminal
    line = line.strip("\n")
    # split line with tab symbol
    line = line.split("\t")
    # convert string to number
    for i in range(len(line)):
      line[i] = float(line[i])
    
    line = np.expand_dims(np.array(line), axis=0)
    # print(np.array(line).shape)
    if (first == True):
      train_label = line
      first = False
    else:
      train_label = np.concatenate((train_label, line), axis = 0)

print(np.squeeze(train_label).shape)
train_label = np.squeeze(train_label)

train_dataset = tf.data.Dataset.from_tensor_slices((train_set, train_label))
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(4, input_dim=3, activation='relu'),
    tf.keras.layers.Dense(4, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
                loss=tf.keras.losses.MeanSquaredError(),
                metrics=[tf.keras.metrics.MeanAbsoluteError(), tf.keras.metrics.MeanSquaredError()])

model.fit(train_dataset, epochs=30)

test_set = []
first = True
with open("/content/test_data.txt", "r") as f:
  for line in f:
    # remove newline terminal
    line = line.strip("\n")
    # split line with tab symbol
    line = line.split("\t")
    # convert string to number
    for i in range(len(line)):
      line[i] = float(line[i])
    
    line = np.expand_dims(np.array(line), axis=0)
    # print(np.array(line).shape)
    if (first == True):
      test_set = line
      first = False
    else:
      test_set = np.concatenate((test_set, line), axis = 0)

print(test_set.shape)

test_dataset = tf.data.Dataset.from_tensor_slices(test_set)

test_dataset = test_dataset.batch(BATCH_SIZE)

prediction = model.predict(test_dataset)

print(type(prediction))
np.savetxt("test_result.txt", prediction)